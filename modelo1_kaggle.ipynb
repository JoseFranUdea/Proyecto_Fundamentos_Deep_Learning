{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCCDpaJRHcR2HWW/ZN2cP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseFranUdea/Proyecto_Fundamentos_Deep_Learning/blob/main/modelo1_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Este cuaderno se puede ejecutar en kaggle, con los datos de la competencia plant pathology 2021"
      ],
      "metadata": {
        "id": "XMdXDLp6Q2re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**En este cuaderno solo se tienen resultados para poder subir a la competencia**"
      ],
      "metadata": {
        "id": "xF0pBdCLRAX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from skimage import exposure\n",
        "import cv2 as cv\n",
        "import os\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import os\n",
        "import multiprocessing as mproc\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"ejecutado\")"
      ],
      "metadata": {
        "id": "1JlI2xlHQ8nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpucheck = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpucheck:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
        "tf.test.is_gpu_available()\n",
        "\n",
        "print(\"ejecutado\")"
      ],
      "metadata": {
        "id": "L9aRVqiFRGgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/kaggle/input/resized-plant2021/img_sz_256/'  #../input/resized-plant2021/img_sz_256/'\n",
        "test_dir =  '/kaggle/input/plant-pathology-2021-fgvc8/test_images/'\n",
        "df = pd.read_csv('../input/plant-pathology-2021-fgvc8/train.csv')\n",
        "\n",
        "print(\"direcciones cargadas\")"
      ],
      "metadata": {
        "id": "A81fWNhsRIdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = glob(train_dir + '/*.jpg')\n",
        "test_files = glob(test_dir + '/*.jpg')\n",
        "print('# files for train',len(train_files))\n",
        "print('# files for train',len(test_files))"
      ],
      "metadata": {
        "id": "io9QA3rxRLs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_batch(path,image_ids, labels):\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    for ind, (image_id, label) in enumerate(zip(image_ids, labels)):\n",
        "        plt.subplot(3, 3, ind + 1)\n",
        "        image = cv.imread(os.path.join(path, image_id))\n",
        "\n",
        "        if image is not None:\n",
        "         image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "         plt.imshow(image)\n",
        "         plt.title(f\"Tipo: {label}\", fontsize=12)\n",
        "         plt.axis(\"off\")\n",
        "         plt.show()\n",
        "        else:\n",
        "          print(\"Debido a que el dataset se redujo, no se pudo cargar la imagen, vuelva a ejecutar.\")\n",
        "\n",
        "\n",
        "ts = df.sample(1)  #Solo mostraremos una\n",
        "image_ids = ts[\"image\"].values\n",
        "labels = ts[\"labels\"].values\n",
        "\n",
        "visualize_batch(train_dir,image_ids,labels)"
      ],
      "metadata": {
        "id": "54l7dFolRRUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_labels = list(itertools.chain([lbs for lbs in df['labels']]))\n",
        "figure(figsize=(12, 6), dpi=80)\n",
        "\n",
        "ax = sns.countplot(y=sorted(all_labels), orient='x')\n",
        "ax.grid()\n",
        "\n",
        "print(df.labels.value_counts())"
      ],
      "metadata": {
        "id": "6pGyC-_kRVJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting labels to binary attributes\n",
        "#Se crea una copia de train\n",
        "train = df.copy()\n",
        "#Se crea una lista de etiquetas para la copia de train\n",
        "train['labels'] = df['labels'].apply(lambda string: string.split(' '))\n",
        "\n",
        "#Transforma las etiquetas para en un formato binario\n",
        "s = list(train['labels'])\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "#crea el data frame trainx aplicando las transformaciones de s, los nombres de las columnas se\n",
        "# se obtienen con mlb.classes, los indices se mantienen de la misma manera que los índices de train\n",
        "\n",
        "trainx = pd.DataFrame(mlb.fit_transform(s), columns=mlb.classes_, index=train.index)\n",
        "#agrega la columna de imagenes del dataframe de train a trainx\n",
        "trainx['image'] = train['image']\n",
        "\n",
        "\n",
        "#Se verifica coom estan los datos\n",
        "\n",
        "print(trainx.head(10))\n",
        "print(trainx.columns)"
      ],
      "metadata": {
        "id": "Mx_ZEPtqRXLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "TARGET_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "DATA_LIMIT = 12000\n",
        "trainx = trainx[:DATA_LIMIT] "
      ],
      "metadata": {
        "id": "I-U101oaRZot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "train_image = []\n",
        "trainxN = []\n",
        "Y = []\n",
        "\n",
        "for i in tqdm(range(trainx.shape[0])):\n",
        "    try:\n",
        "        img_path = os.path.join(train_dir, trainx['image'][i])\n",
        "        \n",
        "        #Check if the image file exists\n",
        "        if not os.path.isfile(img_path):\n",
        "            print(f\"Imagen no encontrada {img_path}. Saltando a la siguiente imagen.\")\n",
        "            continue\n",
        "            \n",
        "        trainxN.append(trainx.iloc[i])\n",
        "        \n",
        "        img = image.load_img(img_path, target_size=(TARGET_SIZE, TARGET_SIZE, 3))\n",
        "        img = image.img_to_array(img)\n",
        "        img = img / 255\n",
        "        train_image.append(img)\n",
        "        \n",
        "        y = trainx.iloc[i].drop(['image'], axis=0)\n",
        "        Y.append(y)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar imagen {img_path}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "X = np.array(train_image)  #Formato de imagen\n",
        "Y = np.array(Y)\n",
        "Y64 = Y.astype(np.int64)\n",
        "trainxNdf = pd.DataFrame(trainxN)  #Formato de tabla"
      ],
      "metadata": {
        "id": "a1_BwwMhRc5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y64, test_size=0.1)"
      ],
      "metadata": {
        "id": "m4Kwo6nMRfi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(TARGET_SIZE,TARGET_SIZE,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "k8YPdtRrRiSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "a7jnPhHERlkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the occuring as the class predicted by model.predict() will be valid\n",
        "name = {0: 'complex',\n",
        "        1: 'frog_eye_leaf_spot',\n",
        "        2: 'healthy',\n",
        "        3: 'powdery_mildew',\n",
        "        4: 'rust',\n",
        "        5: 'scab'}\n",
        "\n",
        "threshold = {0: 0.3,\n",
        "             1: 0.3,\n",
        "             2: 0.4,\n",
        "             3: 0.3,\n",
        "             4: 0.3,\n",
        "            5:0.3}"
      ],
      "metadata": {
        "id": "qMP7L2RaRrGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The class labels are concatenated into a string and written to the final submission.csv dataframe\n",
        "im_labels = []\n",
        "\n",
        "for z in test_files:  #test files son los de prueba que nos proporcionó kaggle\n",
        "    \n",
        "    img = image.load_img(z,target_size=(TARGET_SIZE,TARGET_SIZE,3))\n",
        "    img = image.img_to_array(img)\n",
        "    img = img/255\n",
        "    \n",
        "    proba = list(model.predict(img.reshape(1,TARGET_SIZE,TARGET_SIZE,3))[0])\n",
        "    filename = z.split('/')[-1]\n",
        "    p = []\n",
        "    for i in range(len(proba)):\n",
        "        if proba[i] > threshold[i]:\n",
        "            p.append(name[i])\n",
        "            \n",
        "    im_labels.append({'image': filename, 'labels': ' '.join(p)})\n",
        "    \n",
        "df = pd.DataFrame(im_labels)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MbsBspvxRtRk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}